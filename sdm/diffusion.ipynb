{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F \n",
    "#import attention SelfAttention,CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time_embedding=TimeEmbedding(320)\n",
    "        self.unet=UNET()\n",
    "        self.final=UNET_OutputLayer(320,4)\n",
    "    def forward(self,latent,context,time):\n",
    "        time=self.time_embedding(time)\n",
    "        otuput=self.unet(latent,context,time)\n",
    "\n",
    "        otuput=self.final(otuput)\n",
    "\n",
    "        return otuput   \n",
    "\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self,n_embd):\n",
    "        super().__init__()\n",
    "        self.linear_1=nn.Linear(n_embd,4*n_embd)\n",
    "        self.linear_2=nn.Linear(4*n_embd,4*n_embd)\n",
    "    def forward(self,x):\n",
    "        # x(1,320)\n",
    "        x=self.linear_1(x)\n",
    "        x=F.silu(x)\n",
    "        x=self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchSeqential(nn.Sequential):\n",
    "    def forward(self,x,context,time):\n",
    "        for layer in self:\n",
    "            if isinstance(layer,UNET_AttentionBlock):\n",
    "                x=layer(x,context)\n",
    "            elif isinstance(layer,UNET_ResidualBlock):\n",
    "                x=layer(x,time)\n",
    "            else:\n",
    "                x=layer(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_AttentionBlock(nn.Module):\n",
    "    def __init__(self,n_heads:int,n_embed:int,d_context=768):\n",
    "        super().__init__()\n",
    "        channels=n_heads*n_embed\n",
    "\n",
    "        self.groupnorm=nn.GroupNorm(32,channels,eps=1e-6)\n",
    "        self.conv_input=nn.Conv2d(channels,channels,kernel_size=1,padding=0)\n",
    "\n",
    "\n",
    "        self.layernorm_1=nn.LayerNorm(channels)\n",
    "        self.attention_1=SelfAttentin(n_heads,channels,in_proj_bias=False)\n",
    "        self.layernorm_2=nn.LayerNorm(channels)\n",
    "        self.attention_2=CrossAttention(n_heads,channels,d_context,in_proj_bias=False)\n",
    "        self.layernorm_3=nn.LayerNorm(channels)\n",
    "        self.linear_geglu_1=nn.Linear(channels,4*channels*2)\n",
    "\n",
    "        self.linear_geglu_2=nn.Linear(4*channels,channels)\n",
    "\n",
    "        self.conv_output=nn.Conv2d(channels,channels,kernel_size=1,padding=0)\n",
    "\n",
    "    def forward(self,x,context):\n",
    "        # x:batch,features,height,width\n",
    "        # y:context batch_size,seq_len,dim\n",
    "\n",
    "        resdiue_long=x\n",
    "\n",
    "        x=self.groupnorm(x)\n",
    "\n",
    "        x=self.conv_input(x)\n",
    "\n",
    "        b,c,h,w=x.shape\n",
    "\n",
    "        x=x.view(b,c,h*w)\n",
    "\n",
    "        x=x.transpose(-1,-2)\n",
    "\n",
    "        resdiue_short=x\n",
    "        \n",
    "        x=self.layernorm_1(x)\n",
    "        x=self.attention_1(x)\n",
    "\n",
    "        x+=resdiue_short\n",
    "\n",
    "        resdiue_short=x\n",
    "\n",
    "        #normalizaion cross attention with context and residue\n",
    "\n",
    "        x=self.layernorm_2=(x)\n",
    "        x=self.attention_2(x,context)\n",
    "\n",
    "        x+=resdiue_short\n",
    "\n",
    "        resdiue_short=x\n",
    "\n",
    "        # normalization then ffn with giglu and skip connection\n",
    "\n",
    "        x=self.layernorm_3(x)\n",
    "        # shape will batch_size,pixel=height*width,features turns in 8 times the features\n",
    "\n",
    "        x,gate=self.linear_geglu_1(x).chunk(2,dim=-1)\n",
    "        # two tensor shape of batch_size,pixcels,features/embedding\n",
    "        x=x*F.gelu(gate)\n",
    "        # this layers low the no of features\n",
    "        x=self.linear_geglu_2(x)\n",
    "\n",
    "        # batch_size,height*width,features\n",
    "        x+=resdiue_short\n",
    "        x=x.transpose(-1,-2)\n",
    "        x=x.view(b,c,h,w)\n",
    "\n",
    "        return self.conv_output(x)+resdiue_long\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,n_time=1280):\n",
    "        super().__init__()\n",
    "        self.groupnorm_feature=nn.GroupNorm(32,in_channels)\n",
    "        self.conv_feature=nn.Conv2d(out_channels,kernel_size=3,padding=1)\n",
    "        self.linear_time=nn.Linear(n_time,out_channels)\n",
    "\n",
    "        self.groupnorm_merged=nn.GroupNorm(32,out_channels)\n",
    "        self.conv_merged=nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
    "\n",
    "        if in_channels==out_channels:\n",
    "            self.residual=nn.Identity\n",
    "        else:\n",
    "            self.residual=nn.Conv2d(in_channels,out_channels,kernel_size=1,padding=0)\n",
    "    def forward(self,features,time):\n",
    "            # here the features is latent space and time is what time step it is \n",
    "        residue=features\n",
    "\n",
    "        features=self.groupnorm(features)\n",
    "        features=F.silu(features)\n",
    "\n",
    "            # convert to same channes if in and out not match increase the no of channels\n",
    "        features=self.conv_feature(features)\n",
    "\n",
    "        time=F.silu(time)\n",
    "        time=self.linear_time(time)\n",
    "        # time dimmesin does not include batch_size,channles dimenssion\n",
    "        merged=features+time.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        merged=self.groupnorm_merged(merged)\n",
    "        merged=F.silu(merged)\n",
    "        merged=self.conv_merged(merged)\n",
    "\n",
    "        return merged+self.residual(residue)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init_()\n",
    "        self.encoder=nn.ModuleList([\n",
    "            SwitchSeqential(nn.Conv2d(4,320,kernel_size=3,padding=1)),\n",
    "            SwitchSeqential(UNET_ResidualBlock(320,320),UNET_AttentionBlock(8,40)),\n",
    "            SwitchSeqential(UNET_ResidualBlock(320,320),UNET_AttentionBlock(8,40)),\n",
    "            SwitchSeqential(nn.Conv2d(320,320,kernel_size=3,stride=2,padding=1)),\n",
    "            SwitchSeqential(UNET_ResidualBlock(320,640),UNET_AttentionBlock(8,40)),\n",
    "            SwitchSeqential(UNET_ResidualBlock(640,640),UNET_AttentionBlock(8,40)),\n",
    "            SwitchSeqential(nn.Conv2d(640,640,kernel_size=3,stride=2,padding=1)),\n",
    "            SwitchSeqential(UNET_ResidualBlock(640,1280),UNET_AttentionBlock(8,40)),\n",
    "            SwitchSeqential(UNET_ResidualBlock(1280,1280),UNET_AttentionBlock(8,40)),\n",
    "            SwitchSeqential(nn.Conv2d(1280,1280,kernel_size=3,stride=2,padding=1)),\n",
    "            SwitchSeqential(UNET_ResidualBlock(1280,1280)),\n",
    "            SwitchSeqential(UNET_ResidualBlock(1280,1280)),\n",
    "            # \n",
    "            # bottel_neck layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ])\n",
    "        self.bottleneck=SwitchSeqential(UNET_ResidualBlock(1280,1280),\n",
    "                                        UNET_AttentionBlock(8,160),\n",
    "                                        UNET_ResidualBlock(1280,1280))\n",
    "        self.decoders = nn.ModuleList([\n",
    "            # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
    "            SwitchSeqential(UNET_ResidualBlock(2560, 1280)),\n",
    "            \n",
    "            # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
    "            SwitchSeqential(UNET_ResidualBlock(2560, 1280)),\n",
    "            \n",
    "            # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 32, Width / 32) \n",
    "            SwitchSeqential(UNET_ResidualBlock(2560, 1280), Upsample(1280)),\n",
    "            \n",
    "            # (Batch_Size, 2560, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)\n",
    "            SwitchSeqential(UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)),\n",
    "            \n",
    "            # (Batch_Size, 2560, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)\n",
    "            SwitchSeqential(UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)),\n",
    "            \n",
    "            # (Batch_Size, 1920, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 16, Width / 16)\n",
    "            SwitchSeqential(UNET_ResidualBlock(1920, 1280), UNET_AttentionBlock(8, 160), Upsample(1280)),\n",
    "            \n",
    "            # (Batch_Size, 1920, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)\n",
    "            SwitchSeqential(UNET_ResidualBlock(1920, 640), UNET_AttentionBlock(8, 80)),\n",
    "            \n",
    "            # (Batch_Size, 1280, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)\n",
    "            SwitchSeqential(UNET_ResidualBlock(1280, 640), UNET_AttentionBlock(8, 80)),\n",
    "            \n",
    "            # (Batch_Size, 960, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 8, Width / 8)\n",
    "            SwitchSeqential(UNET_ResidualBlock(960, 640), UNET_AttentionBlock(8, 80), Upsample(640)),\n",
    "            \n",
    "            # (Batch_Size, 960, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "            SwitchSeqential(UNET_ResidualBlock(960, 320), UNET_AttentionBlock(8, 40)),\n",
    "            \n",
    "            # (Batch_Size, 640, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "            SwitchSeqential(UNET_ResidualBlock(640, 320), UNET_AttentionBlock(8, 40)),\n",
    "            \n",
    "            # (Batch_Size, 640, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "            SwitchSeqential(UNET_ResidualBlock(640, 320), UNET_AttentionBlock(8, 40)),\n",
    "        ])\n",
    "    def forward(self,x,context,time):\n",
    "        # x=batch-,4,h/8,w/8\n",
    "        #context=barch,seq_len,dim\n",
    "        # time=1,1280\n",
    "        skip_connection=[]\n",
    "        for layers in self.encoder:\n",
    "            x=layers(x,context,time)\n",
    "            skip_connection.append(x)\n",
    "        x=self.bottleneck(x,context,time)\n",
    "\n",
    "        for layers in self.decoders:\n",
    "            x=torch.cat(x,skip_connection.pop(),dim=1)\n",
    "            x=layers(x,context,time)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_OutputLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(32, in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (Batch_Size, 320, Height / 8, Width / 8)\n",
    "\n",
    "        # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "        x = self.groupnorm(x)\n",
    "        \n",
    "        # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "        x = F.silu(x)\n",
    "        \n",
    "        # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # (Batch_Size, 4, Height / 8, Width / 8) \n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
